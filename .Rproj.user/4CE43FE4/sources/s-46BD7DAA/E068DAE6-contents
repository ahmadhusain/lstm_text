---
title: "Text Classification with LSTM"
author: "Ahmad Husain Abdullah"
date: "`r format(Sys.Date(), '%B %e, %Y')`"
output:
  html_document:
    theme: yeti
    highlight: breezedark
    toc: true
    toc_depth: 3
    toc_float:
      collapsed: true
      smooth_scroll: true
    number_section: true
    df_print: paged
---

```{r setup, include=FALSE}
# clear-up the environment
rm(list = ls())

# chunk options
knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  fig.align = "center"
)
```

# Deep Neural Network

Before we further discuss the Long Short-Term Memory Model, we will first discuss the term of *Deep learning* where the main idea is on the Neural Network. So Neural Network is one branch of machine learning where the learning process imitates the way neurons in the human brain works. In Neural Network we know several terms, such as the input layer, hidden layer, and output layer. So the different betweetn Deep Learning and Neural Network architecture is the number of hidden layers specified. Simple Neural Network only has 1 hidden layer, whereas Deep Learning has more than 1 hidden layer.

```{r echo=FALSE, out.width='80%'}
knitr::include_graphics('img/dnn.png')
```

Nerual Network architecture can also be characterized based on the direction of signal in training process: *feed-forward* and *recurrent*. In this material, we will discuss more the *Recurrent Neural Network* architecture.

## Vanishing / Exploding Gradient Descent

Bacpropogation process aim to update weight and bias in neural network architecture. 

# Recurrent Neural Network

## Model types RNN

## RNN Concept

### Forward Pass

### Backpropogation Through Time

# LSTM

## LSTM Network

# Implementation LSTM with Keras

```{r}
# load packages required
library(keras)
library(RVerbalExpressions)
library(magrittr)
library(textclean)
library(tidyverse)
library(tidytext)
library(rsample)
library(yardstick)
library(caret)

#set seed keras for reproducible result
use_session_with_seed(2)

# set conda env
use_condaenv("tensorflow")
```

## Import Data

```{r}
data <- read_csv("data_input/tweets.csv")
glimpse(data)
```

```{r}
set.seed(100)
subset <- initial_split(data = data, prop = 0.5, strata = "airline_sentiment")
data <- training(subset)
```


```{r}
head(data, 10)
```

## Text Pre-Processing

### Setup regex {.tabset}

#### mention

```{r}
mention <- rx() %>% 
  rx_find(value = "@") %>% 
  rx_alnum() %>% 
  rx_one_or_more()
mention
```

```{r}
"@VirginAmerica What @dhepburn said." %>% 
  str_remove_all(pattern = mention) %>% 
  str_squish()
```

#### hashtag

```{r}
hashtag <- rx() %>% 
  rx_find(value = "#") %>% 
  rx_alnum() %>% 
  rx_one_or_more()
hashtag
```


```{r}
"@VirginAmerica I'm #elevategold for a good reason: you rock!!" %>% 
  str_remove_all(pattern = mention) %>%
  str_remove_all(pattern = hashtag) %>% 
  str_squish()
```


#### question mark

```{r}
question <- rx() %>% 
  rx_find(value = "?") %>% 
  rx_one_or_more()
question
```

#### exclamation mark

```{r}
exclamation <- rx() %>% 
  rx_find(value = "!") %>% 
  rx_one_or_more()
exclamation
```

#### punctuation

```{r}
punctuation <- rx_punctuation()
punctuation
```

#### number

```{r}
number <- rx_digit()
number
```

#### dollar sign

```{r}
dollar <- rx() %>% 
  rx_find("$")
dollar
```


### Text Cleansing {.tabset}

#### `replace_url`

```{r}
"@VirginAmerica Really missed a prime opportunity, there. https://t.co/mWpG7grEZP" %>% 
  replace_url()
```

#### `replace_emoticon`

```{r}
"@SouthwestAir thanks! Very excited to see it :3" %>%
  replace_emoticon()
```

#### `replace_contruction`

```{r}
"@united I'd thank you - but you didn't help. taking 6 hours to reply isn't actually helpful" %>% 
  replace_contraction()
```

#### `replace_word_elongation`

```{r}
"@VirginAmerica heyyyy guyyyys.. :/" %>% 
  replace_word_elongation()
```


```{r}
data <- data %>% 
  mutate(
    text_clean = text %>% 
      replace_url() %>% 
      replace_emoji() %>% 
      replace_emoticon() %>% 
      replace_html() %>% 
      str_remove_all(pattern = mention) %>% 
      str_remove_all(pattern = hashtag) %>% 
      replace_contraction() %>% 
      replace_word_elongation() %>% 
      str_replace_all(pattern = question, replacement = "questionmark") %>% 
      str_replace_all(pattern = exclamation, replacement = "exclamationmark") %>% 
      str_remove_all(pattern = punctuation) %>% 
      str_remove_all(pattern = number) %>% 
      str_remove_all(pattern = dollar) %>% 
      str_to_lower() %>% 
      str_squish()
  )
```

```{r}
data %>% 
  select(text, text_clean) %>% 
  sample_n(20)
```

### prepare datainput

```{r}
data <- data %>% 
  mutate(label = factor(airline_sentiment, levels = c("negative", "neutral", "positive")),
         label = as.numeric(label),
         label = label - 1) %>% 
  select(text_clean, label) %>% 
  na.omit()
head(data, 10)
```

## Tokenizer 

```{r}
num_words <- 1024 

# prepare tokenizers
tokenizer <- text_tokenizer(num_words = num_words,
                            lower = TRUE) %>% 
  fit_text_tokenizer(data$text_clean)
```

```{r}
paste("number of unique words:", length(tokenizer$word_counts))
```

### Intuition

```{r}
docs = c('Well done!',
        'Good work',
        'Great effort',
        'nice work',
        'Excellent!')
tokendocs <- text_tokenizer(num_words = 4, 
                            lower = TRUE) %>% 
  fit_text_tokenizer(docs)
```

```{r}
paste("number of unique words",length(tokendocs$word_counts))
```

```{r}
tokendocs$word_index[1:4]
```

## Splitting Data

```{r}
set.seed(100)
intrain <- initial_split(data = data, prop = 0.8, strata = "label")

data_train <- training(intrain)
data_test <- testing(intrain)

set.seed(100)
inval <- initial_split(data = data_test, prop = 0.5, strata = "label")

data_val <- training(inval)
data_test <- testing(inval)
```


```{r}
maxlen <- max(str_count(data$text_clean, "\\w+")) + 1 
paste("maxiumum length words in data:", maxlen)
```

```{r}
# prepare x
data_train_x <- texts_to_sequences(tokenizer, data_train$text_clean) %>%
  pad_sequences(maxlen = maxlen)

data_val_x <- texts_to_sequences(tokenizer, data_val$text_clean) %>%
  pad_sequences(maxlen = maxlen)

data_test_x <- texts_to_sequences(tokenizer, data_test$text_clean) %>%
  pad_sequences(maxlen = maxlen)

# prepare y
data_train_y <- to_categorical(data_train$label, num_classes = 3)
data_val_y <- to_categorical(data_val$label, num_classes = 3)
data_test_y <- to_categorical(data_test$label, num_classes = 3)
```


### Intuition

```{r}
texts_to_sequences(tokendocs, c("Excellent!", 
                                "Good job bro, keep hard work", 
                                "well done")) %>% 
  pad_sequences(maxlen = 5)
```

```{r}
tokendocs$word_index[3]
```


## Architecture

### Embedding Layer

### Deep Neural Layer

### Output Layer

### Activation Function

### Random Initialization

```{r}
# initiate keras model sequence
model <- keras_model_sequential()

# model
model %>%
  # layer input
  layer_embedding(
    name = "input",
    input_dim = num_words,
    input_length = maxlen,
    output_dim = 32, 
    embeddings_initializer = initializer_random_uniform(minval = -0.05, maxval = 0.05, seed = 2)
  ) %>%
  # layer dropout
  layer_dropout(
    name = "embedding_dropout",
    rate = 0.5
  ) %>%
  # layer lstm 1
  layer_lstm(
    name = "lstm",
    units = 256,
    dropout = 0.2,
    recurrent_dropout = 0.2,
    return_sequences = FALSE, 
    recurrent_initializer = initializer_random_uniform(minval = -0.05, maxval = 0.05, seed = 2),
    kernel_initializer = initializer_random_uniform(minval = -0.05, maxval = 0.05, seed = 2)
  ) %>%
  # layer lstm 2
  layer_lstm(
    name = "lstm",
    units = 128,
    dropout = 0.2,
    recurrent_dropout = 0.2,
    return_sequences = FALSE, 
    recurrent_initializer = initializer_random_uniform(minval = -0.05, maxval = 0.05, seed = 2),
    kernel_initializer = initializer_random_uniform(minval = -0.05, maxval = 0.05, seed = 2)
  ) %>%
  # layer output
  layer_dense(
    name = "output",
    units = 3,
    activation = "softmax", 
    kernel_initializer = initializer_random_uniform(minval = -0.05, maxval = 0.05, seed = 2)
  )
```


### Lost Function

### Optimizer

```{r}
# compile the model
model %>% compile(
  optimizer = "adam",
  metrics = "accuracy",
  loss = "categorical_crossentropy"
)

# model summary
summary(model)
```


## Train the Model

```{r}
# model fit settings
epochs <- 10
batch_size <- 128

# fit the model
history <- model %>% fit(
  data_train_x, data_train_y,
  batch_size = batch_size, 
  epochs = epochs,
  verbose = 1,
  validation_data = list(
    data_val_x, data_val_y
  )
)

# history plot
plot(history)
```


## Model Evaluation

```{r}
# predict on train
data_train_pred <- model %>%
  predict_classes(data_train_x) %>%
  as.vector()

# predict on val
data_val_pred <- model %>%
  predict_classes(data_val_x) %>%
  as.vector()

# predict on test
data_test_pred <- model %>%
  predict_classes(data_test_x) %>%
  as.vector()
```


```{r}
# accuracy on data train
accuracy_vec(
 truth = factor(data_train$label,labels = c("negative", "neutral", "positive")),
 estimate = factor(data_train_pred, labels = c("negative", "neutral", "positive"))
)
```


```{r}
# accuracy on data test
accuracy_vec(
 truth = factor(data_test$label,labels = c("negative", "neutral", "positive")),
 estimate = factor(data_test_pred, labels = c("negative", "neutral", "positive"))
)
```

